{
  
    
        "post0": {
            "title": "Create and train a model from scratch !",
            "content": "What will we do ? . We will create a linear model then a neural network from scratch to do binary classification, train them using gradient descent and finally see how current libraries sipmlify these things for us and we&#39;ll create our own class that mimicks the tools provided by these libraries. . If you never used Pytorch before, i would refer you to my other post Start simple, start with a baseline !, which gives a more gentle introduction to the tools needed here. . Getting and preparing the data (you can skip this part) . We&#39;ll be using the MNIST_SAMPLE dataset provided by fastai, which contains images of 3&#39;s and 7&#39;s. . We start by downloading the data, opening the images, turning them into tensors and stacking them into a rank-3 tensors (matrix) for each label (train/valid) separately. . path = untar_data(URLs.MNIST_SAMPLE) three_train = torch.stack([tensor(Image.open(o)).float()/255 for o in (path/&#39;train&#39;/&#39;3&#39;).ls()]) seven_train = torch.stack([tensor(Image.open(o)).float()/255 for o in (path/&#39;train&#39;/&#39;7&#39;).ls()]) seven_valid = torch.stack([tensor(Image.open(o)).float()/255 for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) three_valid = torch.stack([tensor(Image.open(o)).float()/255 for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) . Then, we&#39;ll concatenate 3/7 tensors obtained (train/ valid seperately) into one single tensor and flatten the images out by reshaping the tensors. . For our independant variables we&#39;ll create the corresponding tensor, with 1 indicating a 3 and 0 indicating a 7. Add a dimension, to not have problems further down the road (due to broadcasting). . train_x = torch.cat([three_train, seven_train]).view(-1, 28*28) train_y = tensor([1] * len(three_train) + [0] * len(seven_train)).unsqueeze(1) train_x.shape, train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . valid_x = torch.cat([three_valid, seven_valid]).view(-1, 28*28) valid_y = tensor([1] * len(three_valid) + [0] * len(seven_valid)).unsqueeze(1) valid_x.shape, valid_y.shape . (torch.Size([2038, 784]), torch.Size([2038, 1])) . Create our dataset (for PyTorch it is a list of tuples containing our dependant/ independant variables). . Then create our dataloader, which is obtained by shuffling the dataset, and creating batches of size 256. . And wrap that in a Dataloaders object. . dset = list(zip(train_x, train_y)) dl = DataLoader(dset, batch_size=256, shuffle=True) dset_valid = list(zip(valid_x, valid_y)) dl_valid = DataLoader(dset_valid, batch_size=256, shuffle=True) dls = DataLoaders(dl, dl_valid) . Create and train the model . 1. From scratch approach . Okay, now that we&#39;ve done the kinda &quot;boring&quot; part, let&#39;s dive into our subject of interest. . To assess how good our model is (human consumption), we&#39;ll need to define a metric. . def batch_accuracy(predictions, targets): return ((predictions &gt; 0) == targets).float().mean() . Now we also need to define a loss function, that we will optimize using SGD. . def mnist_loss(predictions, targets): result = predictions.sigmoid() return torch.where(targets == 1, 1 - result , result).mean() . Where&#39;s the model at ? No worries, he&#39;s just here waiting for you ! . In python &quot;@&quot; refers to matrix multiplication. . def linear(xb): return xb @ w + b . Something to randomly initialize the parameters with. And initialize them. . def init_params(*size): return (torch.randn(size)).requires_grad_() w, b = init_params(28*28, 1), init_params(1) . We need a training loop that corresponds to the graph below. So let&#39;s do just that. . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop epochs = 5 # setting the learning rate lr = 1 # Training loop for i in range(epochs): for xb, yb in dls[0]: # Predict result = linear(xb) # Calculate the loss loss = mnist_loss(result, yb) # Calculate the gradient loss.backward() # Take a step w.data -= w.grad * lr b.data -= b.grad * lr # Set the gradient to zero, s the next time we calculate it, it doesn&#39;t accumulate w.grad.zero_() b.grad.zero_() # Show the batch accuracy for each epoch print(tensor([batch_accuracy(linear(xb), yb) for xb, yb in dls[1]]).mean().item(), end=&#39; &#39;) . 0.9121212959289551 0.9434308409690857 0.9494490027427673 0.9557371139526367 0.9602308869361877 . Now let&#39;s scale up a bit, and go for the neural net. Which will consist of two linear layers (first and last) and one non-linearity between them (Which in this case is the rectified linear unit). . def simple_net(xb): result1 = xb @ w1 + b1 result2 = F.relu(result1) return result2 @ w2 + b2 . w1, b1 = init_params(28*28, 30), init_params(30) w2, b2 = init_params(30, 1), init_params(1) . We&#39;ll be using the same training loop, but since it&#39;s a more &quot;complex&quot; model, we&#39;ll lower the learning rate and train for more epochs. So we&#39;ll just show the accuracy for the last epoch. . epochs = 40 # setting the learning rate lr = 0.1 # Training loop for i in range(epochs): for xb, yb in dls[0]: # Predict result = simple_net(xb) # Calculate the loss loss = mnist_loss(result, yb) # Calculate the gradient loss.backward() # Take a step w1.data -= w1.grad * lr b1.data -= b1.grad * lr w2.data -= w2.grad * lr b2.data -= b2.grad * lr # Set the gradient to zero, s the next time we calculate it, it doesn&#39;t accumulate w1.grad.zero_() b1.grad.zero_() w2.grad.zero_() b2.grad.zero_() # Show the batch accuracy for the last epoch print(tensor([batch_accuracy(simple_net(xb), yb) for xb, yb in dls[1]]).mean().item(), end=&#39; &#39;) . 0.953414797782898 . 2. Using existing tools . Okay, but all of what we&#39;ve done so far can already be implemented using fastai2 and PyTorch using just these lines of code. . simple_net = nn.Sequential( nn.Linear(28*28, 30), nn.ReLU(), nn.Linear(30, 1) ) # Create the Learner object learn = Learner(dls, simple_net, loss_func=mnist_loss, opt_func=SGD, metrics=batch_accuracy) # Train the model learn.fit(40, lr = 0.1) . As you can see we create a Learner object, it&#39;s a class that handles all the training process for you, you just give it five things: . Your dataloaders object. | The model. | The loss function to optimize. | The optimization function. | The metrics to be displayed (Optionnal). | . The optimization function (Optimizer) in PyTorch is a function that handles the gradient step for you, e.g. updating the params and setting the gradient to zero. . opt = SGD(simple_net.parameters(), 0.1) # we give it the params and learning rate . Let&#39;s try to mimik this by creating our own Learner class from scratch. . class MyLearner: # Initializing the learner def __init__(self, dls, model, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy): self.dls = dls self.model = model self.opt_func = opt_func self.loss_func = loss_func self.metrics = metrics # We store the metric values in a list (to plot them for example) self.metric_values = [] # Method for training our model def fit(self, epochs, lr=1): # Create the optimizer opt = self.opt_func(self.model.parameters(), lr) # Training loop (same as before) for i in range(epochs): for xb, yb in self.dls[0]: result = self.model(xb) loss = self.loss_func(result, yb) loss.backward() # We update the weights using our optimizer opt.step() # Setting the gradient to zero using the same optimizer opt.zero_grad() # Calculate the metric value, store it and print it for each epoch b_accuracy = tensor([self.metrics(self.model(xb), yb) for xb, yb in self.dls[1]]).mean().item() self.metric_values.append(b_accuracy) print(round(b_accuracy, 4), end=&#39; &#39;) . Let&#39;s try it for a simple linear model. IT WORKS !!! . my_learner = MyLearner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) # Train it my_learner.fit(20) # Plot the metric values with each epoch plt.plot(my_learner.metric_values); . 0.9706 0.9749 0.977 0.9779 0.9784 0.9789 0.9794 0.9808 0.9793 0.9798 0.9803 0.9803 0.9813 0.9808 0.9813 0.9814 0.9823 0.9829 0.9823 0.9824 .",
            "url": "https://igrek-code.github.io/blog/2020/11/06/model-from-scratch.html",
            "relUrl": "/2020/11/06/model-from-scratch.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Start simple, start with a baseline !",
            "content": "Motivation . Don&#39;t start with something fancy, complicated as your first solution. Always start with a baseline (simple solution). Why ? You might ask, well there&#39;s two reasons for that : . So you have something to compare to. | Because complicated does not imply effective (your simple solution might be better then the fancy one). | . If you are using colab, make sure to change your runtime to GPU. . What will we do ? . We will to implement a simple approach to handwritten digit classification without Deep Learning. We&#39;ll see that it is pretty accurate, and we&#39;ll learn some fastai, pytorch, computer vision basics along the way. . How will we do it ? . We will use a dataset called MNIST, which is a pretty popular dataset containing images of handwritten digits but we&#39;ll use a sample of it containing only the digits 3 and 7 (which is provided by fastai). . Our approach consists of two steps: . Obtain the image of perfect 3 and 7. | Compare every picture we have to these perfect 3 and 7, and to whom ever it is closest, we&#39;ll attribute it the corresponding label. | . Let&#39;s do it ! . 1. Create our perfect 3 and 7 . Okay, before we start let&#39;s download our dataset. For that we will use the fastai method untar_data, which given an url, downloads the dataset (if not already downloaded), and extracts it (if not already exctracted). We position our selves in the dataset folder by setting the BASE_PATH, to the path returned by the method. . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . Inside that folder we can see two other folders valid and train, that contain the validation and training set respectively. You&#39;ll see this pretty frequently when you download publically availble datasets (they will be already be split into a valid and training sets for you). . path.ls() . (#3) [Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;),Path(&#39;train&#39;)] . Now inside the train folder, we have also two other folders 3 and 7 that as you probably guess it contain the respective images of the digits (This is also pretty common in public datasets, the different images will be put in folders named with the corresponding label). . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/3&#39;),Path(&#39;train/7&#39;)] . We get the paths of the different images (sorted), to open them for later. If you notice, there is (#6131) before the list, this is because it&#39;s not an actual list but rather a fastai object called L, which is a list but with some additional features (another feature is that it displays only the first ten elements of the list). . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . Let&#39;s open one image for each label, to ensure that they are actuall images of 3 and 7. . im3_path = threes[1] im3 = Image.open(im3_path) im3 . im7_path = sevens[4] im7 = Image.open(im7_path) im7 . Images are numbers ? . Before we go any further, i need to tell you that an image for a computer is just a bunch of numbers. . To be more precise, it is a composed of pixels. To simplify things (and this is the case for our dataset), we&#39;ll take an example of gray scale images (images containing only black, white, and shades of grey in between). Every pixel makes a tiny bit of the image where the pixel takes a value (between 0 and 255) and that value tells to the computer how to display that particular pixel. Aseembled together they form the image. . Okay, let&#39;s turn our image into a Numpy array (which is just an array but wth super powers, and Numpy is just the most used python module for Numerical computing). . The thing between brackets is called slicing, and it&#39;s telling Numpy that we want to display the fourth row/column up to but not including the thenth one. . As we can see, an image is just a bunch of numbers. . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . We&#39;ll do the same thing, but instead of an array, we&#39;ll turn our image into a PyTorch tensor, which behaves much like a Numpy array but has the benefit to do our operations in the GPU (it&#39;s a lot, i mean a lot faster). . And the dtype in the end is the type of data in the tensor/array, which in this case is an unsigned 8 bit integer (0 to 255). . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . To better visualize how a computer display an image, we&#39;ll turn our tensor containing the pixel values of our image into a DataFrame (it&#39;s an object provided by the Pandas module. For now, you just need to know that it&#39;s a table of values). With each value (pixel) in the table displayed with the corresponding color (0 for white, 255 for black, values in between for shades of grey). . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . Let&#39;s open all our images, turn them into tensors and put them in the corresponding list. To speed things up, we&#39;ll use a list comprehension (for more details a quick Google search will do it). . Let&#39;s ensure that we have all our images ready by checking the length of the two lists. . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . So far we have seen how to turn our images into tensors/arrays (a bunch of numbers), but how do we do the reverse operation ? . Well it is pretty simple, thanks to fastai show_image method, we can do just that. . show_image(three_tensors[1]); . Remember we have to find the perfect 3 and 7, one way of doing it (our approach) is to find the mean of every pixel of all the 3/7 images of our dataset. . To do that we&#39;ll stack our second order tensors (second order means that it&#39;s a two dimensionnal object or matrix, more generally we say that we have a k-th order tensor) to form a third order tensor (three dimesionnal object). And cast it to float (turn our integers in the tensor into float, in other words change the type of the data in our tensor). . We also divide by 255 because when images are represented by floats, we generally expect the values to be between 0 and 255. . To ensure that we&#39;ve done this right we print the shape of the tensor (which is the number of elements in each axis/dimension) and we check that we have indeed 6131 images of 28x28. . Note that nothing about this tensor tells us explicitly that the first axis is the number of images and so on. This is because it is entirely up to us and how we construct the it. For the tensor it is just a bunch of numbers in memory. . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . We can also check that we did not screw something, by checking the number of dimensions/axis of the tensor (which in this case is three). . The number of dimmesions/axis of the tensor is also the length of the shape. . stacked_threes.ndim . 3 . Okay, now we have everything we need, to calculate our perfect 3/7 we&#39;ll use the mean method provided by PyTorch and we give it as an argument the index along which we want to calculate the mean, in our case it&#39;ll be the first axis (0-indexed) because we want the mean of every pixel across all images. . mean3 = stacked_threes.mean(0) show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . As you can notice, the parts where images &quot;agree&quot; (similar pixel values) that this is how a handwritten 3/7 is supposed to look like are darker then the parts where they disagree-- kinda of blurry (due to different pixel values). . 2. Measure the distance (similarity) . Our final step will consist of calculating the distance of a given image and our perfect 3/7 and we&#39;ll say that it is a 3/7 if it is closest to the perfect 3/7 respectively. . Okay, but what&#39;s the distance between two images. For that purpose we can use either : . L1 norm: which is the mean absolute difference, in other words, we take the absolute value of the difference between each pixel of the two images and average over all pixels. . | L2 norm: root mean squared error, same as above but instead of taking the absolute value, we square the differences, then average and finally take the square root (which will &quot;undo&quot; the squaring). . | . a_3 = stacked_threes[1] dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs, dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs, dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . Taking a random 3, we see that the distance from the perfect 3 is less then the distance from the perfect 7, which is exactly what we wanted. Our solution looks promising ! . PyTorch provides methods to calculate L1 norm &amp; MSE (MSE is the just L2 norm whitout the square root), they are inside the torch.nn.functional, which is already imported by fastai as F. . L1 &amp; L2 norm are loss functions. . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . The MSE penalyzes mistakes more heavily then L1 norm (on the other hand it is more merciful towards small mistakes). . To test how good our model is, we&#39;ll need a validation set (images he has not seen before), recall that in our dataset folder this is already provided by MNIST in the valid folder. So we&#39;ll just do the same operations we did for the training set seen above to create our tensors. . It&#39;s good practise to check your tensor&#39;s shape to ensure that you&#39;ve done everything properly. . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Let&#39;s create a function that does what we&#39;ve discussed earlier (calculate distances), the (-1, -2) means that we calculate the mean along the last axis and the second last axis (which represents the width and the height of the image). . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . Cool this was for one image tho but how about all the validation set (because this is what we need to calulate the metric). Well i&#39;am glad you ask, this shows that you are paying attention (or just that i&#39;am talking to myself ...). . Well thanks to PyTorch we don&#39;t have to re-write mnist_distance function because it uses a neat trick called Broadcasting. It means that rather then complaining about the shapes being different, when given a rank 3 tensor (validation set images) and rank 2 tensor (ideal 3), it treats the rank 2 tensor as if it was a rank 3 one (you can imagine in your head that it kinda of duplicates the ideal 3 1010 times and it substracts it from each image from the validation set element wise, but it does not actually duplicate it-- no more memory is allocated). . Finally it takes the mean over the last and second last axis, which is the height and width of each image, which leaves us with rank 1 tensor (array), of the distance of each image in the validation set from the ideal 3. . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1133, 0.1656, 0.1848, ..., 0.1623, 0.1447, 0.1252]), torch.Size([1010])) . Now all is left to do is create a function that tells us whether an image is a 3 or a 7. By comparing the distance between the ideal 3/7 respectively (True for 3, False for 7). . This function will automaticlly do Broadcasting and be applied element wise, just like all PyTorch functions and operators. . def is_3(x): return mnist_distance(x, mean3) &lt; mnist_distance(x, mean7) . is_3(valid_3_tens) . tensor([ True, True, True, ..., True, False, True]) . If we convert a boolean tensor into float we get a 1. for True and 0. for False (this will come in handy to calculate the accuracy). . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . Let&#39;s see how good our simple solution is. It has 95% overall accuracy which is more then acceptable ! . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) .",
            "url": "https://igrek-code.github.io/blog/2020/11/01/start-simple-start-with-a-baseline.html",
            "relUrl": "/2020/11/01/start-simple-start-with-a-baseline.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Quick prototyping with fastai2 (Car classifier example)",
            "content": "What we will do ? . We will build a simple car classfier, you basicly give it an image of a car (interior or exterior) and the model will &#39;guess&#39; what brand it is and if it is the interior or exterior of it. Okay, let&#39;s do this. . Here&#39;s a live demo of the model: You just upload an image, click classify and voilà, you have your prediction. . Before we start . I will not go into every detail of the code, so prior knowledge of python is required. Also make sure that you are using a GPU (for colab just click Runtime -&gt; Change runtime type -&gt; GPU). That said we will first start by importing the tools that we will need to do the job. . 1. Get the data . To train a model, we will need data that it uses to learn from (it&#39;s like a student learning from a book, where the student is the model and the book is the data). For that we will download our car images using the Bing Api Key, to get one i will refer to this great post by Ivoline Ngong on the fastai forums: Get bing api key. Okay, now that we got our key we will use it to download the images in this case, we will download Audi/ BMW/ Mercedes (interior/exterior) images and put them in different folders accroding to their category (we will call &#39;categories&#39; labels from now on, this is the jargon used in deep learning) . key = &#39;INSERT_YOUR_KEY_HERE&#39; path = Path(&#39;/content/images&#39;) brands = [&#39;bmw&#39;, &#39;mercedes&#39;, &#39;audi&#39;] perspective = [&#39;interior&#39;, &#39;exterior&#39;] result = itertools.product(brands, perspective) if not path.exists(): path.mkdir() for b, p in result: dest = Path(path/f&#39;{b}_{p}&#39;) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{b} {p}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . We then check if there are any corrupted files in the mix, we use verify_images for that (which is provided by fastai), and we obviously unlink (delete) them, so we don&#39;t encounter errors further down the road. . You can notice that we use an intresting method which is provided by fastai, get_images_files, which is used as the name suggests to search for images (with different extensions) in a given path (input), this is done recursively by default (searches in all sub-directories of the path given) and returns a list of paths of those images. . fns = get_image_files(path) failed = verify_images(fns) failed.map(Path.unlink) . (#5) [None,None,None,None,None] . 2. Clean the data . Now comes the &quot; NO, GOD PLEASE NO &quot; moment, but will try to make it a bit more &quot;fun&quot;. . The intuitive approach will be to do data cleaning before we train our model, but what we will do instead is train a quick and simple model to help us clean the data. . Don&#39;t worry about the code for now, we&#39;ll explain how train a model later. . db = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.1, seed=42), get_y=parent_label, item_tfms=Resize(244)) dls = db.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 2.345028 | 1.059248 | 0.413793 | 00:31 | . epoch train_loss valid_loss error_rate time . 0 | 1.212386 | 1.005042 | 0.356322 | 00:31 | . 1 | 1.053597 | 1.014616 | 0.298851 | 00:32 | . 2 | 0.831190 | 0.910729 | 0.275862 | 00:32 | . 3 | 0.692817 | 0.914627 | 0.287356 | 00:31 | . If we plot our top losses (the images for which the model got the wrong guess), we see that he was not that wrong after all. I mean, taking the first image it looks like a bmw exterior to me. . Well this is due to downloading images from bing, when you do a search for images (in any search engine) you will get the desired images that you searched for but sometimes, you&#39;ll get some &quot; undesired &quot; ones as well. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(2, nrows=2) . So what do we do now ? Good question, we will use a tool provided by fastai to clean our data, which is called ImageClassifierCleaner. And it provides us with a nice GUI interface that we can interact with to delete/ keep (by default) /or modify the label of the images in the dataset. . The ImageClassifierCleaner does not allow us to delete or modify the labels directly but rather gives a list of the paths of the different images that we selected. . You will have to iterate this process untill you have cleaned your dataset (e.g. Re-train -&gt; plot top losses -&gt; clean) . cleaner = ImageClassifierCleaner(learn) cleaner . In this example, we will change the labels (category) from bmw interior to bmw exterior. . for idx,cat in cleaner.change(): cleaner.fns[idx].rename(path/cat/f&#39;(1){cleaner.fns[idx].name}&#39;) . 3. Training . Before training our model we will need to turn our downloaded data into a DataLoaders object (which is just an object containing our train and validation set, e.g. the images used for training and the images used to test how well our model performs), we need to tell fastai at least four things: . What kinds of data we are working with: blocks=(ImageBlock, CategoryBlock), we are dealing with images and we want categories as the output. | How to get the list of items get_items=get_image_files (remember this is the method we described earlier) | How to label these items . get_y=parent_label, this means we label them according to the folder they&#39;re in. | How to create the validation set splitter=RandomSplitter(valid_pct=0.1, seed=42), we take 10% of our images and use them to test how good our model performs, and set the seed so we always have the same split. | . We also are going to be using a technique called Data Augmentation, which is just a set of standard transformations applied to our images (rotation, contrast, brightness, ..., all are contained in the aug_transforms method), this is usually done to increase the size of our dataset, when you have a small one (not a lot of images) like our case. . item_tfms=RandomResizedCrop(244, min_scale=0.5), batch_tfms=aug_transforms() . Now the difference between item_tfms and batch_tfms, is just that one is applied one item at a time, the other is applied to a batch of images at a time (this is where the GPU comes in handy, beacause it can do multiple tasks in parallel, the CPU does this sequencially). . Note that we also resize all of our images into 244x244 format, so all our images are of the same size (but you can choose any size you want). . db = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.1, seed=42), get_y=parent_label, item_tfms=Resize(244)) dls = db.dataloaders(path) . Now, to train our model we will use a technique called transfor learning, which basicly means to use an already pre-trained model (which was trained for some task) for a different task that we are intrested in (classifiying cars). . We will call the method cnn_learner for this purpose, we put our Dataloaders object as the first parameter, the architecture as the second (which is the &quot;mathematical template of our function&quot; in this case Resnet, 34 just means the number of layers), and last argument is the metric, which is the thing that we use as humans to evaluate how good the model performs (in this case error_rate, which is the ratio of number of misclassified images / total number of images). . Finally, we use the method fine_tune, which will do two things : . Do one epoch (one pass through the dataset) to ajust the parts of the model to get the new random head working (the head is the last layer, we change it because we are using our model for a different task than it was used before). | Use the number of epochs requested (in this case 10) to fit (train) the entire model, and updating the last layers especially the head faster then the earlier ones (because when using an already pre-trained model, the first layers already &#39;detect&#39; basic faatures like straight lines, and we are interested in updating the last ones because they are responsible of sophisticated features like detecting wheels, car doors, ...). | . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(10) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 2.339309 | 1.290998 | 0.505747 | 00:32 | . epoch train_loss valid_loss error_rate time . 0 | 1.294172 | 0.868396 | 0.321839 | 00:32 | . 1 | 1.035632 | 0.653016 | 0.241379 | 00:33 | . 2 | 0.823092 | 0.467765 | 0.137931 | 00:33 | . 3 | 0.627784 | 0.434415 | 0.137931 | 00:32 | . 4 | 0.491646 | 0.489917 | 0.160920 | 00:32 | . 5 | 0.384105 | 0.437487 | 0.137931 | 00:32 | . 6 | 0.303227 | 0.383987 | 0.103448 | 00:32 | . 7 | 0.244072 | 0.372062 | 0.080460 | 00:33 | . 8 | 0.199574 | 0.364913 | 0.080460 | 00:33 | . 9 | 0.167494 | 0.368371 | 0.080460 | 00:33 | . After training our model, we can plot the confusion matrix, which will give us better insight of how well our model is doing in classfiying the different images based on their actual label. . You want to have dark blues in the diagonal, this means that your model is classifying the different images correctly. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Inference . Now, to really test our model, we will test it using images that he has not seen before, or else it would be conidered cheating. It&#39;s like a student passing an exam, he will not be given questions that he already saw before to but rather will probably get questions that he had not seen before, but in the context of what he studied. . When using a model to get predictions, we call it Inference. . For that we&#39;ll be using the predict method, that just takes the image as the input. And returns three things, the prediction (what car it thinks it is), the &quot;posistion&quot; of the label in the vocab, and the probability for each label. . img = PILImage.create(&#39;/content/bmw_exterior_2.jpg&#39;) img.to_thumb(244) . predict, position, proba = learn.predict(img) predict, position, proba . (&#39;bmw_exterior&#39;, tensor(2), tensor([9.8757e-08, 2.9618e-10, 1.0000e+00, 1.8380e-09, 4.8084e-06, 4.3692e-08])) . Our model thinks it&#39;s a BMW exterior with 100% certainty, PERFECT !!! . To export our model for later use, or integrate it in an app, or whatever have you, we just use the export method. Which will export the model as a pickle file, with all the learned weights and how it should load the data (so you just import, plug in the image and you get the results). . We use load_learner to load the model. . learn.export() .",
            "url": "https://igrek-code.github.io/blog/2020/10/28/fastai-car-classifier.html",
            "relUrl": "/2020/10/28/fastai-car-classifier.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Drivetrain approach with example (Designing better data products)",
            "content": "What is the Drivetrain approach ? . It is basicly a process to design data products, to ensure that your modeling work is useful in practise. It was introduced in 2012 by Jeremy, along with Margit Zwemer and Mike Loukides. You can find out more about it here: Designing good data products . Ok, but what does it consist of ? . Well, now that we saw what it was usefull for, and where it came from, let’s see the four steps that compose it 1. . . We need to define the objective (the end goal) | Next we need to identify the levers (what can we control) | After that collect relevent data | Finally train our model | Example . No good explanation is made without a concrete example, so let’s do that. . As a graduate student i will be attending classes, with other students, but as of today we are living in the COVID-19 crysis. And i don’t want to catch it. . Let’s say i’am a director of the computer science department. And i want to minimize the spread of COVID-19 across students of my department (this is the objective). . One of the things i can controll, is to make sure everyone is wearing a face mask, by installing for example cameras across the department entries that would detect if you are wearing a mask or not and would let you or not enter by say opening the door or locking it (this is the lever). . Now to make such a system, i’ll need to train a model for image recognition, so i’ll collect data which will consist of people wearing masks and others not (that’s the data phase). . Finally come the ‘fun’ part where you actually train your model and get to see it in action. . Reference . This blog post was highly influenced by the book Deep Learning for Coders with fastai &amp; PyTorch, in fact i’am writing this blog as part of the homework. If there is any problem of copyright regarding the use of the picture or anything else please let me know. &#8617; . |",
            "url": "https://igrek-code.github.io/blog/2020/10/26/drive-train-approach.html",
            "relUrl": "/2020/10/26/drive-train-approach.html",
            "date": " • Oct 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://igrek-code.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://igrek-code.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}