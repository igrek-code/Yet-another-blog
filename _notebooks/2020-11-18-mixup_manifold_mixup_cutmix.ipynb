{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "variation_of_mixup_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggBjJzpB-o29"
      },
      "source": [
        "# Mixup, Manifold Mixup and CutMix \n",
        "\n",
        "- toc: true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFBeGQs48nu0"
      },
      "source": [
        "#hide\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastbook import *\n",
        "from fastai.vision.all import *\n",
        "path = untar_data(URLs.PETS)\n",
        "pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
        "                 get_items=get_image_files, \n",
        "                 splitter=RandomSplitter(seed=42),\n",
        "                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
        "                 item_tfms=Resize(460),\n",
        "                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
        "dls = pets.dataloaders(path/\"images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_re6W7v554pA"
      },
      "source": [
        "## What will we do ?\n",
        "\n",
        "In this post, we will see three data augmentation techniques: MixUp, Manifold MixUp and Cutmix. And how to implement them in fastai2.\n",
        "\n",
        "> To implement them in fastai2, we'll use callbacks.\n",
        "\n",
        "> Callbacks are what is used inside fastai to inject custom behavior in the training loop (we use 'cbs=' argument inside the Learner).\n",
        "\n",
        "> Note that these techniques require far more epochs to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFiw3opA_xP5"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "Data augmentation is a technique used to enrich your dataset, by creating synthetic data generated from your original data.\n",
        "\n",
        "This is used as a regularization technique, in other means your model will generalize better to new data that he has not seen before. \n",
        "\n",
        "DNN have a problem, they predict overconfidently stuff, so with regularization we have better genralization and smoother decision boundary (transition from one class to the other).\n",
        "\n",
        "The three techniques that we'll see provide an additional benefit that is your model will get more robust to corrupted labels and adverserial attacks.\n",
        "\n",
        "> Adverserial attacks are noise added to your input, inperceptible by the human eye but will confuse your model to producing totally different predictions from the ground truth labels. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOnstUxZ8DMZ"
      },
      "source": [
        "## 1. Mixup\n",
        "\n",
        "The idea is pretty basic :\n",
        "\n",
        "1. Select two samples (images in our case) at random from the dataset.\n",
        "2. Select a weight at random (from the Beta distribution where alpha is a hyperparameter, i.e. λ ∼ Beta(α, α)).\n",
        "3. Take a weighted average of the two samples previously chosen using that weight, this is your independent variable.\n",
        "4. Take a weighted average of the targets of your corresponding images, with the same weight this will be your dependant variable.\n",
        "\n",
        "Then you continue the usual procedure for training your DNN.\n",
        "\n",
        "> Your targets must be one-hot-encoded.\n",
        "\n",
        "> The related paper: [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsBcdm4TDe4K"
      },
      "source": [
        "Here's the code implementation of it as described in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81WqJxwE9MHh"
      },
      "source": [
        "\n",
        "# Take two samples at a time\n",
        "for (x1, y1), (x2, y2) in zip(loader1, loader2):\n",
        "\n",
        "  # Select a random weight\n",
        "  lam = numpy.random.beta(alpha, alpha)\n",
        "\n",
        "  # Take the weighted average of the two samples, to obtain your independent variable\n",
        "  x = Variable(lam * x1 + (1. - lam) * x2)\n",
        "\n",
        "  # Take the weighted average of the targets, to obtain your dependent variable\n",
        "  y = Variable(lam * y1 + (1. - lam) * y2)\n",
        "\n",
        "  # Continue training as usual\n",
        "  optimizer.zero_grad()\n",
        "  loss(net(x), y).backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YALf1i94SOVb"
      },
      "source": [
        "MixUp is already available as a Callback in fastai2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofWnqzBcSkJE"
      },
      "source": [
        "\n",
        "# Import callback\n",
        "from fastai.callback.mixup import *\n",
        "# Create model\n",
        "learn_with_mixup = Learner(dls, resnet18(), metrics=accuracy, cbs = MixUp(0.2))\n",
        "# Train model\n",
        "learn_with_mixup.fit_one_cycle(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhdUthgwFR8b"
      },
      "source": [
        "Now let's see the remaining two techniques, which are just variants of this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmaDShaL92Dc"
      },
      "source": [
        "## 2. Manifold mixup\n",
        "\n",
        "It's the same idea as MixUp but instead of just applying it for the input (i.e. before the first layer), we can apply this *mixing* before another layer of the model.\n",
        "\n",
        "So the procedure goes, as described in the paper :  \n",
        "1. we select a random layer k from a set of eligible layers S in the\n",
        "neural network. This set may include the input layer. \n",
        "2. we process two random data minibatches (x1, y1) and (x2, y2) as usual, until reaching layer k. This provides us with two intermediate minibatches (result_1, y1) and (result_2, y2). \n",
        "3. we perform MixUp on these intermediate minibatches. This produces the mixed minibatch (mixed_result, mixed_y).\n",
        "4. we continue the forward pass in the network from layer k until the\n",
        "output using the mixed minibatchs. \n",
        "5. this output is used to compute the loss value and\n",
        "gradients that update all the parameters of the neural network.\n",
        "\n",
        "> The related paper: [Manifold Mixup: Better Representations by Interpolating Hidden States](https://arxiv.org/abs/1806.05236)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb06ryx4_T9X"
      },
      "source": [
        "Here's a pseudo-code to understand the implementation. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVaEbZylLSUE"
      },
      "source": [
        "\n",
        "# Creating a simple network \n",
        "first_layer = nn.Linear(20, 10)\n",
        "second_layer = nn.Relu()\n",
        "third_layer = nn.Linear(10, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDYW3Ys4hAwW"
      },
      "source": [
        "\n",
        "# Pick eligible layers before training\n",
        "eligible = first_layer, third_layer\n",
        "\n",
        "# Take two samples at a time\n",
        "for (x1, y1), (x2, y2) in zip(loader1, loader2):\n",
        "\n",
        "# Choose a layer at random (in the paper it was from a uniform distribution)\n",
        "  chosen = choose_random(eligile) \n",
        "\n",
        "# Let's say chosen == third_layer\n",
        "\n",
        "# Forward pass through first and second layer\n",
        "  result_xb1 = second_layer(first_layer(xb1))\n",
        "  result_xb2 = second_layer(first_layer(xb2))\n",
        "\n",
        "# Select a random weight\n",
        "  lam = numpy.random.beta(alpha, alpha)\n",
        "\n",
        "# Take the weighted average of the two intermediate results\n",
        "  new_result = result_xb1 * lam + result_xb2 * (1 - lam)\n",
        "\n",
        "# Take the weighted average of the targets, to obtain your dependent variable\n",
        "  y = yb1 * lam + yb2 * (1 - lam)\n",
        "\n",
        "# Continue forward pass with the new_result\n",
        "  pred = third_layer(new_result)\n",
        "\n",
        "# Continue training as usual\n",
        "  optimizer.zero_grad()\n",
        "  loss(pred, y).backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYbeztmYHpGn"
      },
      "source": [
        "Now this one is not availble in fastai2, but no worries there is a module created by a guy called Nestor Demeure from the fastai community who implemented the manifold mixup as a Callback for us to use in fastai2.\n",
        "\n",
        "Github repo of the module : [ManifoldMixupV2](https://github.com/nestordemeure/ManifoldMixupV2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAvtGvDOpYHT"
      },
      "source": [
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/nestordemeure/ManifoldMixupV2.git\n",
        "# Copy the module in the current directory\n",
        "!cp /content/ManifoldMixupV2/manifold_mixup.py /content/\n",
        "# Import the module\n",
        "from manifold_mixup import *\n",
        "# Create the model\n",
        "learn_with_Mmixup = Learner(dls, resnet18(), metrics=accuracy, cbs=ManifoldMixup(0.2))\n",
        "# Train the model\n",
        "learn_with_Mmixup.fit_one_cycle(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fah8pWkq_4lq"
      },
      "source": [
        "## 3. Cutmix\n",
        "\n",
        "Finally the most recent of the three, which again follows the same procedure as MixUp (is applied to input only) but instead of *mixing* the random samples, we cut a portion of one sample and patch it into the other, this will be your independent variable. \n",
        "\n",
        "Now for your dependent variable, you take the propotion of each sample appearing in the new one created.  \n",
        "\n",
        "Here's an image from the paper that will help you understand.\n",
        "\n",
        "![Description of CutMix. Image from the original paper](../images/cutmix_image_example.jpg)\n",
        "\n",
        "> The related paper: [CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://arxiv.org/abs/1905.04899)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYORlUUqJsNF"
      },
      "source": [
        "CutMix is also availble as a Callback in fastai2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlm88flKPM1_"
      },
      "source": [
        "\n",
        "# Import callback\n",
        "from fastai.callback.cutmix import * \n",
        "# Create the model\n",
        "learn_cut_mix = Learner(dls, resnet18(), metrics=accuracy, cbs=CutMix(2.0))\n",
        "# Train the model\n",
        "learn_cut_mix.fit_one_cycle(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}